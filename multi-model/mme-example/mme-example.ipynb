{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon SageMaker Multi-Model Endpoints using Scikit Learn\n",
    "This notebook shows an example of using multi-model endpoints for **predicting housing prices** at different locations. The predicitve models are trainined in Scikit Learn and will be deployed behind a Sagemaker multi-model endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assumption: I already have the list of trained model for 4 cities in the US\n",
    "model_list = ['LosAngeles_CA', 'Chicago_IL', 'Houston_TX', 'NewYork_NY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mYou are using pip version 10.0.1, however version 20.0.1 is available.\r\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "# First I install the dependencies\n",
    "!pip install -qU awscli boto3 sagemaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set names for S3 Prefix, Model uploading artifact, ECR Image name, Endpoint Instance Type\n",
    "\n",
    "First get the role and session.\n",
    "The S3 bucket name and the S3 data prefix associated with the multi-model endpoint is declared here. If the bucket does not exist it is also created. Note that a default bucket is created using sagemaker sessions to ensure the right roles are attached.\n",
    "The script_file has the function declaration for loading the model in the container and the the user_code_artifact is the tar.gz file packages this code and uploads to the S3 bucket later.\n",
    "Also declared is the name of the ECR image for deploying the model.\n",
    "The instance type of the endpoint will be an ml.m4.xlarge instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.predictor import csv_serializer\n",
    "import boto3\n",
    "\n",
    "sm_client = boto3.client(service_name='sagemaker')\n",
    "runtime_sm_client = boto3.client(service_name='sagemaker-runtime')\n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = get_execution_role()\n",
    "\n",
    "# S3 Prefix\n",
    "BUCKET      = sagemaker_session.default_bucket()\n",
    "DATA_PREFIX            = 'DEMO_MME_SCIKIT'\n",
    "MULTI_MODEL_ARTIFACTS  = 'multi_model_artifacts'\n",
    "\n",
    "# Additional API: Artifact for uploading model from S3 to Container\n",
    "SCRIPT_FILENAME     = 'inference.py'\n",
    "USER_CODE_ARTIFACTS = 'user_code.tar.gz'\n",
    "\n",
    "# ECR Image name\n",
    "ALGORITHM_NAME = 'multi-model-sklearn'\n",
    "ACCOUNT_ID  = boto3.client('sts').get_caller_identity()['Account']\n",
    "REGION      = boto3.Session().region_name\n",
    "MULTI_MODEL_SKLEARN_IMAGE = '{}.dkr.ecr.{}.amazonaws.com/{}:latest'.format(ACCOUNT_ID, REGION, \n",
    "                                                                           ALGORITHM_NAME)\n",
    "# ECR Image and Model Object Name\n",
    "HOUSING_MODEL_NAME     = 'ushousing'\n",
    "ENDPOINT_INSTANCE_TYPE = 'ml.m4.xlarge'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build and register a Scikit Learn container that can serve multiple models\n",
    "\n",
    "This is the script for creating the image that will serve the multi-model endpoint.\n",
    "This image if it not in ECR will be created and pushed to ECR.\n",
    "For an inference container to serve multiple models in a multi-model endpoint, we must implement [additional APIs](https://docs.aws.amazon.com/sagemaker/latest/dg/build-multi-model-build-container.html) in order to load, list, get, unload and invoke specific models.\n",
    "\n",
    "We refer to the 'mme' branch of the [SageMaker Scikit Learn Container repository](https://github.com/aws/sagemaker-scikit-learn-container/tree/mme) which shows an example implementation on how to adapt SageMaker's Scikit Learn framework container to use [Multi Model Server](https://github.com/awslabs/multi-model-server). MMS is a framework that provides an HTTP frontend that implements the additional container APIs required by multi-model endpoints, and also provides a pluggable backend handler for serving models using a custom framework, in this case the Scikit Learn framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Login Succeeded\n",
      "sha256:33fac7063dad5e17683ecce178e56dac4c6f09ead19ff4f53453b6e2167a2903\n",
      "running bdist_wheel\n",
      "running build\n",
      "running build_py\n",
      "creating build\n",
      "creating build/lib\n",
      "creating build/lib/sagemaker_sklearn_container\n",
      "copying src/sagemaker_sklearn_container/__init__.py -> build/lib/sagemaker_sklearn_container\n",
      "copying src/sagemaker_sklearn_container/training.py -> build/lib/sagemaker_sklearn_container\n",
      "copying src/sagemaker_sklearn_container/serving.py -> build/lib/sagemaker_sklearn_container\n",
      "copying src/sagemaker_sklearn_container/handler_service.py -> build/lib/sagemaker_sklearn_container\n",
      "creating build/lib/sagemaker_sklearn_container/mms_patch\n",
      "copying src/sagemaker_sklearn_container/mms_patch/mms_transformer.py -> build/lib/sagemaker_sklearn_container/mms_patch\n",
      "copying src/sagemaker_sklearn_container/mms_patch/__init__.py -> build/lib/sagemaker_sklearn_container/mms_patch\n",
      "copying src/sagemaker_sklearn_container/mms_patch/model_server.py -> build/lib/sagemaker_sklearn_container/mms_patch\n",
      "installing to build/bdist.linux-x86_64/wheel\n",
      "running install\n",
      "running install_lib\n",
      "creating build/bdist.linux-x86_64\n",
      "creating build/bdist.linux-x86_64/wheel\n",
      "creating build/bdist.linux-x86_64/wheel/sagemaker_sklearn_container\n",
      "copying build/lib/sagemaker_sklearn_container/__init__.py -> build/bdist.linux-x86_64/wheel/sagemaker_sklearn_container\n",
      "creating build/bdist.linux-x86_64/wheel/sagemaker_sklearn_container/mms_patch\n",
      "copying build/lib/sagemaker_sklearn_container/mms_patch/mms_transformer.py -> build/bdist.linux-x86_64/wheel/sagemaker_sklearn_container/mms_patch\n",
      "copying build/lib/sagemaker_sklearn_container/mms_patch/__init__.py -> build/bdist.linux-x86_64/wheel/sagemaker_sklearn_container/mms_patch\n",
      "copying build/lib/sagemaker_sklearn_container/mms_patch/model_server.py -> build/bdist.linux-x86_64/wheel/sagemaker_sklearn_container/mms_patch\n",
      "copying build/lib/sagemaker_sklearn_container/training.py -> build/bdist.linux-x86_64/wheel/sagemaker_sklearn_container\n",
      "copying build/lib/sagemaker_sklearn_container/serving.py -> build/bdist.linux-x86_64/wheel/sagemaker_sklearn_container\n",
      "copying build/lib/sagemaker_sklearn_container/handler_service.py -> build/bdist.linux-x86_64/wheel/sagemaker_sklearn_container\n",
      "running install_egg_info\n",
      "running egg_info\n",
      "creating src/sagemaker_sklearn_container.egg-info\n",
      "writing src/sagemaker_sklearn_container.egg-info/PKG-INFO\n",
      "writing dependency_links to src/sagemaker_sklearn_container.egg-info/dependency_links.txt\n",
      "writing entry points to src/sagemaker_sklearn_container.egg-info/entry_points.txt\n",
      "writing requirements to src/sagemaker_sklearn_container.egg-info/requires.txt\n",
      "writing top-level names to src/sagemaker_sklearn_container.egg-info/top_level.txt\n",
      "writing manifest file 'src/sagemaker_sklearn_container.egg-info/SOURCES.txt'\n",
      "reading manifest file 'src/sagemaker_sklearn_container.egg-info/SOURCES.txt'\n",
      "reading manifest template 'MANIFEST.in'\n",
      "writing manifest file 'src/sagemaker_sklearn_container.egg-info/SOURCES.txt'\n",
      "Copying src/sagemaker_sklearn_container.egg-info to build/bdist.linux-x86_64/wheel/sagemaker_sklearn_container-1.0-py3.6.egg-info\n",
      "running install_scripts\n",
      "creating build/bdist.linux-x86_64/wheel/sagemaker_sklearn_container-1.0.dist-info/WHEEL\n",
      "creating '/home/ec2-user/SageMaker/multi_model_sklearn_home_value_2020-01-16/sagemaker-scikit-learn-container/dist/sagemaker_sklearn_container-1.0-py2.py3-none-any.whl' and adding '.' to it\n",
      "adding 'sagemaker_sklearn_container/__init__.py'\n",
      "adding 'sagemaker_sklearn_container/handler_service.py'\n",
      "adding 'sagemaker_sklearn_container/serving.py'\n",
      "adding 'sagemaker_sklearn_container/training.py'\n",
      "adding 'sagemaker_sklearn_container/mms_patch/__init__.py'\n",
      "adding 'sagemaker_sklearn_container/mms_patch/mms_transformer.py'\n",
      "adding 'sagemaker_sklearn_container/mms_patch/model_server.py'\n",
      "adding 'sagemaker_sklearn_container-1.0.dist-info/entry_points.txt'\n",
      "adding 'sagemaker_sklearn_container-1.0.dist-info/top_level.txt'\n",
      "adding 'sagemaker_sklearn_container-1.0.dist-info/WHEEL'\n",
      "adding 'sagemaker_sklearn_container-1.0.dist-info/METADATA'\n",
      "adding 'sagemaker_sklearn_container-1.0.dist-info/RECORD'\n",
      "removing build/bdist.linux-x86_64/wheel\n",
      "sha256:e9992a79e7e26c6cc2d371e5690b2e1b17b1d5c3b3728c7a073b3f4d265f752d\n",
      "The push refers to repository [963992372437.dkr.ecr.us-east-1.amazonaws.com/multi-model-sklearn]\n",
      "a2dafd997744: Preparing\n",
      "e4b3646a6851: Preparing\n",
      "1a4051491d09: Preparing\n",
      "908e4b60b16c: Preparing\n",
      "b6e613c5d2c7: Preparing\n",
      "ad6dc8593b13: Preparing\n",
      "240ce14fde7f: Preparing\n",
      "1a6280277d2e: Preparing\n",
      "337824f45a36: Preparing\n",
      "abb4fc2e5b28: Preparing\n",
      "6b4059309231: Preparing\n",
      "4e20f1b6ce45: Preparing\n",
      "77008e118980: Preparing\n",
      "6cb741cb00b7: Preparing\n",
      "f36b28e4310d: Preparing\n",
      "91d23cf5425a: Preparing\n",
      "337824f45a36: Waiting\n",
      "1a6280277d2e: Waiting\n",
      "ad6dc8593b13: Waiting\n",
      "240ce14fde7f: Waiting\n",
      "abb4fc2e5b28: Waiting\n",
      "6b4059309231: Waiting\n",
      "4e20f1b6ce45: Waiting\n",
      "77008e118980: Waiting\n",
      "6cb741cb00b7: Waiting\n",
      "f36b28e4310d: Waiting\n",
      "91d23cf5425a: Waiting\n",
      "b6e613c5d2c7: Pushed\n",
      "e4b3646a6851: Pushed\n",
      "1a4051491d09: Pushed\n",
      "240ce14fde7f: Layer already exists\n",
      "a2dafd997744: Pushed\n",
      "1a6280277d2e: Layer already exists\n",
      "337824f45a36: Layer already exists\n",
      "abb4fc2e5b28: Layer already exists\n",
      "6b4059309231: Layer already exists\n",
      "908e4b60b16c: Pushed\n",
      "77008e118980: Layer already exists\n",
      "6cb741cb00b7: Layer already exists\n",
      "4e20f1b6ce45: Layer already exists\n",
      "f36b28e4310d: Layer already exists\n",
      "91d23cf5425a: Layer already exists\n",
      "ad6dc8593b13: Pushed\n",
      "latest: digest: sha256:3d33a741ecf57cceb9dc07f37db0e996176789159811704b7ae9a377933280e1 size: 3669\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING! Using --password via the CLI is insecure. Use --password-stdin.\n",
      "WARNING! Your password will be stored unencrypted in /home/ec2-user/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n",
      "Cloning into 'sagemaker-scikit-learn-container'...\n"
     ]
    }
   ],
   "source": [
    "%%sh -s $ALGORITHM_NAME\n",
    "\n",
    "algorithm_name=$1\n",
    "\n",
    "account=$(aws sts get-caller-identity --query Account --output text)\n",
    "\n",
    "# Get the region defined in the current configuration (default to us-west-2 if none defined)\n",
    "region=$(aws configure get region)\n",
    "\n",
    "ecr_image=\"${account}.dkr.ecr.${region}.amazonaws.com/${algorithm_name}:latest\"\n",
    "\n",
    "# If the repository doesn't exist in ECR, create it.\n",
    "aws ecr describe-repositories --repository-names \"${algorithm_name}\" > /dev/null 2>&1\n",
    "\n",
    "if [ $? -ne 0 ]\n",
    "then\n",
    "    aws ecr create-repository --repository-name \"${algorithm_name}\" > /dev/null\n",
    "fi\n",
    "\n",
    "# Get the login command from ECR and execute it directly\n",
    "$(aws ecr get-login --region ${region} --no-include-email --registry-ids ${account})\n",
    "\n",
    "# Build the docker image locally with the image name and then push it to ECR\n",
    "# with the full image name.\n",
    "\n",
    "# First clear out any prior version of the cloned repo\n",
    "rm -rf sagemaker-scikit-learn-container/\n",
    "\n",
    "# Clone the sklearn container repo\n",
    "git clone --single-branch --branch mme https://github.com/aws/sagemaker-scikit-learn-container.git\n",
    "cd sagemaker-scikit-learn-container/\n",
    "\n",
    "# Build the \"base\" container image that encompasses the installation of the\n",
    "# scikit-learn framework and all of the dependencies needed.\n",
    "docker build -q -t sklearn-base:0.20-2-cpu-py3 -f docker/0.20-2/base/Dockerfile.cpu --build-arg py_version=3 .\n",
    "\n",
    "# Create the SageMaker Scikit-learn Container Python package.\n",
    "python setup.py bdist_wheel --universal\n",
    "\n",
    "# Build the \"final\" container image that encompasses the installation of the\n",
    "# code that implements the SageMaker multi-model container requirements.\n",
    "docker build -q -t ${algorithm_name} -f docker/0.20-2/final/Dockerfile.cpu .\n",
    "\n",
    "docker tag ${algorithm_name} ${ecr_image}\n",
    "\n",
    "docker push ${ecr_image}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The code used to train the model\n",
    "\n",
    "Here is a script that helps load the model to the container. This script is both for training the model and loading the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting inference.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $SCRIPT_FILENAME\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import glob\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "# inference functions ---------------\n",
    "def model_fn(model_dir):\n",
    "    print('loading model.joblib from: {}'.format(model_dir))\n",
    "    _loaded_model = joblib.load(os.path.join(model_dir, 'model.joblib'))\n",
    "    return _loaded_model\n",
    "\n",
    "\n",
    "if __name__ =='__main__':\n",
    "\n",
    "    print('extracting arguments')\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # hyperparameters sent by the client are passed as command-line arguments to the script.\n",
    "    # to simplify the demo we don't use all sklearn RandomForest hyperparameters\n",
    "    parser.add_argument('--n-estimators', type=int, default=10)\n",
    "    parser.add_argument('--min-samples-leaf', type=int, default=3)\n",
    "\n",
    "    # Data, model, and output directories\n",
    "    parser.add_argument('--model-dir', type=str, default=os.environ.get('SM_MODEL_DIR'))\n",
    "    parser.add_argument('--train', type=str, default=os.environ.get('SM_CHANNEL_TRAIN'))\n",
    "    parser.add_argument('--validation', type=str, default=os.environ.get('SM_CHANNEL_VALIDATION'))\n",
    "    parser.add_argument('--model-name', type=str)\n",
    "\n",
    "    args, _ = parser.parse_known_args()\n",
    "\n",
    "    print('reading data')\n",
    "    print('model_name: {}'.format(args.model_name))\n",
    "\n",
    "    train_file = os.path.join(args.train, args.model_name + '_train.csv')    \n",
    "    train_df = pd.read_csv(train_file)\n",
    "\n",
    "    val_file = os.path.join(args.validation, args.model_name + '_val.csv')\n",
    "    test_df = pd.read_csv(os.path.join(val_file))\n",
    "\n",
    "    print('building training and testing datasets')\n",
    "    X_train = train_df[train_df.columns[1:train_df.shape[1]]] \n",
    "    X_test = test_df[test_df.columns[1:test_df.shape[1]]]\n",
    "    y_train = train_df[train_df.columns[0]]\n",
    "    y_test = test_df[test_df.columns[0]]\n",
    "\n",
    "    # train\n",
    "    print('training model')\n",
    "    model = RandomForestRegressor(\n",
    "        n_estimators=args.n_estimators,\n",
    "        min_samples_leaf=args.min_samples_leaf,\n",
    "        n_jobs=-1)\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # print abs error\n",
    "    print('validating model')\n",
    "    abs_err = np.abs(model.predict(X_test) - y_test)\n",
    "\n",
    "    # print couple perf metrics\n",
    "    for q in [10, 50, 90]:\n",
    "        print('AE-at-' + str(q) + 'th-percentile: '\n",
    "              + str(np.percentile(a=abs_err, q=q)))\n",
    "        \n",
    "    # persist model\n",
    "    path = os.path.join(args.model_dir, 'model.joblib')\n",
    "    joblib.dump(model, path)\n",
    "    print('model persisted at ' + path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Package Inference entry point code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When using multi-model endpoints with the Scikit Learn container, we need to provide an entry point for\n",
    "# inference, a function that will load the saved model in S3 into the container. This function uploads a \n",
    "# the user code artifact containing such a script. This tar.gz file will be fed to the SageMaker multi-model \n",
    "# creation and pointed to by theSAGEMAKER_SUBMIT_DIRECTORY environment variable.\n",
    "\n",
    "def upload_inference_code(script_file_name, prefix):\n",
    "    _tmp_folder = 'inference-code'\n",
    "    if not os.path.exists(_tmp_folder):\n",
    "        os.makedirs(_tmp_folder)\n",
    "    !tar -czvf $_tmp_folder/$USER_CODE_ARTIFACTS $script_file_name > /dev/null\n",
    "    _loc = sagemaker_session.upload_data(_tmp_folder, \n",
    "                                         key_prefix='{}/{}'.format(prefix, _tmp_folder))\n",
    "    return _loc + '/' + USER_CODE_ARTIFACTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Model for multi-model endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thus function creates the model object with the path to the S3 prefix, the ECR image that was created previously\n",
    "# and the path to the code that loads the model into the container\n",
    "def create_multi_model_entity(multi_model_name, role):\n",
    "    # establish the place in S3 from which the endpoint will pull individual models\n",
    "    _model_url  = 's3://{}/{}/{}/'.format(BUCKET, DATA_PREFIX, MULTI_MODEL_ARTIFACTS)\n",
    "    _container = {\n",
    "        'Image':        MULTI_MODEL_SKLEARN_IMAGE,\n",
    "        'ModelDataUrl': _model_url,\n",
    "        'Mode':         'MultiModel',\n",
    "        'Environment': {\n",
    "            'SAGEMAKER_PROGRAM' : SCRIPT_FILENAME,\n",
    "            'SAGEMAKER_SUBMIT_DIRECTORY' : upload_inference_code(SCRIPT_FILENAME, DATA_PREFIX)\n",
    "        }\n",
    "    }\n",
    "    create_model_response = sm_client.create_model(\n",
    "        ModelName = multi_model_name,\n",
    "        ExecutionRoleArn = role,\n",
    "        Containers = [_container])\n",
    "    \n",
    "    return _model_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi model name: ushousing-2020-01-23-04-58-23\n"
     ]
    }
   ],
   "source": [
    "from time import gmtime, strftime\n",
    "import os\n",
    "\n",
    "multi_model_name = '{}-{}'.format(HOUSING_MODEL_NAME, strftime('%Y-%m-%d-%H-%M-%S', gmtime()))\n",
    "model_url = create_multi_model_entity(multi_model_name, role)\n",
    "print('Multi model name: {}'.format(multi_model_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Endpoint Config\n",
    "\n",
    "Next we create an endpoint config that defines the instance type, count variant weight and the name of the model that was created in the previous step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint config name: ushousing-2020-01-23-04-58-23\n",
      "Endpoint name: ushousing-2020-01-23-04-58-23\n"
     ]
    }
   ],
   "source": [
    "endpoint_config_name = multi_model_name\n",
    "print('Endpoint config name: ' + endpoint_config_name)\n",
    "\n",
    "create_endpoint_config_response = sm_client.create_endpoint_config(\n",
    "    EndpointConfigName = endpoint_config_name,\n",
    "    ProductionVariants=[{\n",
    "        'InstanceType': ENDPOINT_INSTANCE_TYPE,\n",
    "        'InitialInstanceCount': 1,\n",
    "        'InitialVariantWeight': 1,\n",
    "        'ModelName'   : multi_model_name,\n",
    "        'VariantName' : 'AllTraffic'}])\n",
    "\n",
    "endpoint_name = multi_model_name\n",
    "print('Endpoint name: ' + endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Endpoint\n",
    "\n",
    "Next the model is deployed by creating the endpoint. The endpoint takes the endpoint config that was created previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint Arn: arn:aws:sagemaker:us-east-1:963992372437:endpoint/ushousing-2020-01-23-04-58-23\n"
     ]
    }
   ],
   "source": [
    "create_endpoint_response = sm_client.create_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    EndpointConfigName=endpoint_config_name)\n",
    "print('Endpoint Arn: ' + create_endpoint_response['EndpointArn'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for ushousing-2020-01-23-04-58-23 endpoint to be in service...\n"
     ]
    }
   ],
   "source": [
    "print('Waiting for {} endpoint to be in service...'.format(endpoint_name))\n",
    "waiter = sm_client.get_waiter('endpoint_in_service')\n",
    "waiter.wait(EndpointName=endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate synthetic test data\n",
    "\n",
    "The models for house price prediction were built using synthetically generated data. We use the same process to generate some test data or inference payload.\n",
    "Each test sample has six attributes namely area of house, num of bedroom, bathrooms, lot acres, garage space and year built. These are linearly combined and added to a base price to derive the price. Goal to predict the price for a new data sample.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_HOUSES_PER_LOCATION = 1000\n",
    "LOCATIONS  = ['NewYork_NY',    'LosAngeles_CA',   'Chicago_IL',    'Houston_TX',   'Dallas_TX',\n",
    "              'Phoenix_AZ',    'Philadelphia_PA', 'SanAntonio_TX', 'SanDiego_CA',  'SanFrancisco_CA']\n",
    "COLUMNS = ['PRICE', 'YEAR_BUILT', 'SQUARE_FEET', 'NUM_BEDROOMS',\n",
    "           'NUM_BATHROOMS', 'LOT_ACRES', 'GARAGE_SPACES']\n",
    "MAX_YEAR = 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_price(house):\n",
    "    _base_price = int(house['SQUARE_FEET'] * 150)\n",
    "    _price = int(_base_price + (10000 * house['NUM_BEDROOMS']) + \\\n",
    "                               (15000 * house['NUM_BATHROOMS']) + \\\n",
    "                               (15000 * house['LOT_ACRES']) + \\\n",
    "                               (15000 * house['GARAGE_SPACES']) - \\\n",
    "                               (5000 * (MAX_YEAR - house['YEAR_BUILT'])))\n",
    "    return _price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_random_house():\n",
    "    _house = {'SQUARE_FEET':   int(np.random.normal(3000, 750)),\n",
    "              'NUM_BEDROOMS':  np.random.randint(2, 7),\n",
    "              'NUM_BATHROOMS': np.random.randint(2, 7) / 2,\n",
    "              'LOT_ACRES':     round(np.random.normal(1.0, 0.25), 2),\n",
    "              'GARAGE_SPACES': np.random.randint(0, 4),\n",
    "              'YEAR_BUILT':    min(MAX_YEAR, int(np.random.normal(1995, 10)))}\n",
    "    _price = gen_price(_house)\n",
    "    return [_price, _house['YEAR_BUILT'],   _house['SQUARE_FEET'], \n",
    "                    _house['NUM_BEDROOMS'], _house['NUM_BATHROOMS'], \n",
    "                    _house['LOT_ACRES'],    _house['GARAGE_SPACES']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uploading 2 Models from Local to S3 Bucket\n",
    "\n",
    "Two of the models from the model list are uploaded to the S3 bucket with the fixed prefix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: ./LosAngeles_CA.tar.gz to s3://sagemaker-us-east-1-963992372437/DEMO_MME_SCIKIT/multi_model_artifacts/LosAngeles_CA.tar.gz\n",
      "upload: ./Chicago_IL.tar.gz to s3://sagemaker-us-east-1-963992372437/DEMO_MME_SCIKIT/multi_model_artifacts/Chicago_IL.tar.gz\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp LosAngeles_CA.tar.gz s3://$BUCKET/$DATA_PREFIX/$MULTI_MODEL_ARTIFACTS/\n",
    "!aws s3 cp Chicago_IL.tar.gz s3://$BUCKET/$DATA_PREFIX/$MULTI_MODEL_ARTIFACTS/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List of models the MME endpoint will have access to\n",
    "\n",
    "This is the list of model in the S3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are the models that the endpoint has at its disposal:\n",
      "2020-01-23 05:15:03  386.9 KiB Chicago_IL.tar.gz\n",
      "2020-01-23 05:15:02  384.3 KiB LosAngeles_CA.tar.gz\n",
      "\n",
      "Total Objects: 2\n",
      "   Total Size: 771.1 KiB\n"
     ]
    }
   ],
   "source": [
    "print('Here are the models that the endpoint has at its disposal:')\n",
    "!aws s3 ls --human-readable --summarize $model_url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Invoke multiple individual models hosted behind a single endpoint\n",
    "\n",
    "Next we show how to invoke the endpoint using this test data using the InvokeEndpoint function that we have seen before. The predict_one house function is predicting the price for 1 house given the model name and payload.\n",
    "\n",
    "Some of the model names are randomly picked and prediction made. Note that we get an error when we are trying to call a model that does not existing in the S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import datetime\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_one_house_value(features, model_name):\n",
    "    print('Using model {} to predict price of this house: {}'.format(full_model_name,\n",
    "                                                                     features))\n",
    "\n",
    "    _float_features = [float(i) for i in features]\n",
    "    _body = ','.join(map(str, _float_features)) + '\\n'\n",
    "    \n",
    "    _start_time = time.time()\n",
    "\n",
    "    _response = runtime_sm_client.invoke_endpoint(\n",
    "                        EndpointName=endpoint_name,\n",
    "                        ContentType='text/csv',\n",
    "                        TargetModel=full_model_name,\n",
    "                        Body=_body)\n",
    "    _predicted_value = json.loads(_response['Body'].read())[0]\n",
    "\n",
    "    _duration = time.time() - _start_time\n",
    "    \n",
    "    print('${:,.2f}, took {:,d} ms\\n'.format(_predicted_value, int(_duration * 1000)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model LosAngeles_CA.tar.gz to predict price of this house: [2008, 4057, 4, 1.0, 0.99, 1]\n",
      "$642,295.23, took 1,037 ms\n",
      "\n",
      "Using model LosAngeles_CA.tar.gz to predict price of this house: [2005, 4062, 5, 2.0, 0.63, 1]\n",
      "$643,161.18, took 123 ms\n",
      "\n",
      "Using model LosAngeles_CA.tar.gz to predict price of this house: [2004, 4550, 2, 2.5, 0.98, 3]\n",
      "$723,025.77, took 114 ms\n",
      "\n",
      "Using model LosAngeles_CA.tar.gz to predict price of this house: [1996, 2470, 4, 1.0, 1.02, 3]\n",
      "$375,575.37, took 115 ms\n",
      "\n",
      "Using model LosAngeles_CA.tar.gz to predict price of this house: [1983, 3064, 6, 2.0, 1.04, 2]\n",
      "$416,198.69, took 114 ms\n",
      "\n",
      "Using model LosAngeles_CA.tar.gz to predict price of this house: [1991, 3235, 4, 1.0, 0.97, 2]\n",
      "$470,995.81, took 118 ms\n",
      "\n",
      "Using model LosAngeles_CA.tar.gz to predict price of this house: [1997, 1924, 6, 1.5, 0.68, 0]\n",
      "$280,092.33, took 115 ms\n",
      "\n",
      "Using model LosAngeles_CA.tar.gz to predict price of this house: [1994, 3294, 5, 1.0, 0.98, 3]\n",
      "$491,854.85, took 116 ms\n",
      "\n",
      "Using model LosAngeles_CA.tar.gz to predict price of this house: [1985, 3420, 3, 1.5, 0.85, 2]\n",
      "$449,794.13, took 114 ms\n",
      "\n",
      "Using model LosAngeles_CA.tar.gz to predict price of this house: [2007, 1840, 4, 2.0, 0.7, 0]\n",
      "$335,677.94, took 114 ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_list2 = ['Chicago_IL', 'LosAngeles_CA']\n",
    "for i in range(10):\n",
    "    model_name = model_list2[np.random.randint(1, len(model_list2))]\n",
    "    full_model_name = '{}.tar.gz'.format(model_name)\n",
    "    predict_one_house_value(gen_random_house()[1:], full_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model Houston_TX.tar.gz to predict price of this house: [2000, 3050, 4, 1.5, 1.12, 1]\n"
     ]
    },
    {
     "ename": "ValidationError",
     "evalue": "An error occurred (ValidationError) when calling the InvokeEndpoint operation: \"Failed to download model data(bucket: sagemaker-us-east-1-963992372437, key: DEMO_MME_SCIKIT/multi_model_artifacts/Houston_TX.tar.gz). Please ensure that there is an object located at the URL and that the role passed to CreateModel has permissions to download the model.\n\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-a9cdba331a88>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mmodel_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mfull_model_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'{}.tar.gz'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mpredict_one_house_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen_random_house\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull_model_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-18-7154fc16978f>\u001b[0m in \u001b[0;36mpredict_one_house_value\u001b[0;34m(features, model_name)\u001b[0m\n\u001b[1;32m     12\u001b[0m                         \u001b[0mContentType\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'text/csv'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m                         \u001b[0mTargetModel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfull_model_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m                         Body=_body)\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0m_predicted_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_response\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Body'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    274\u001b[0m                     \"%s() only accepts keyword arguments.\" % py_operation_name)\n\u001b[1;32m    275\u001b[0m             \u001b[0;31m# The \"self\" in this scope is referring to the BaseClient.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_api_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperation_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m         \u001b[0m_api_call\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpy_operation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    584\u001b[0m             \u001b[0merror_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Error\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Code\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m             \u001b[0merror_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merror_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_response\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValidationError\u001b[0m: An error occurred (ValidationError) when calling the InvokeEndpoint operation: \"Failed to download model data(bucket: sagemaker-us-east-1-963992372437, key: DEMO_MME_SCIKIT/multi_model_artifacts/Houston_TX.tar.gz). Please ensure that there is an object located at the URL and that the role passed to CreateModel has permissions to download the model.\n\""
     ]
    }
   ],
   "source": [
    "# iterate through invocations with random inputs against a random model showing results and latency\n",
    "import numpy as np\n",
    "\n",
    "for i in range(10):\n",
    "    model_name = model_list[np.random.randint(1, len(model_list))]\n",
    "    full_model_name = '{}.tar.gz'.format(model_name)\n",
    "    predict_one_house_value(gen_random_house()[1:], full_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamically deploy another model\n",
    "\n",
    "Nex twe upload the other two models. Note there is no code change for the new models. Note that it takes more time to respond as it is uploading the model the first time. After that latency is much less. So Any one of the models can be invoked using the same nul-model endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: ./Houston_TX.tar.gz to s3://sagemaker-us-east-1-963992372437/DEMO_MME_SCIKIT/multi_model_artifacts/Houston_TX.tar.gz\n",
      "upload: ./NewYork_NY.tar.gz to s3://sagemaker-us-east-1-963992372437/DEMO_MME_SCIKIT/multi_model_artifacts/NewYork_NY.tar.gz\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp Houston_TX.tar.gz s3://$BUCKET/$DATA_PREFIX/$MULTI_MODEL_ARTIFACTS/\n",
    "!aws s3 cp NewYork_NY.tar.gz s3://$BUCKET/$DATA_PREFIX/$MULTI_MODEL_ARTIFACTS/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are the models that the endpoint has at its disposal:\n",
      "2020-01-23 05:15:03     396157 Chicago_IL.tar.gz\n",
      "2020-01-23 05:17:30     396349 Houston_TX.tar.gz\n",
      "2020-01-23 05:15:02     393494 LosAngeles_CA.tar.gz\n",
      "2020-01-23 05:17:31     395990 NewYork_NY.tar.gz\n"
     ]
    }
   ],
   "source": [
    "print('Here are the models that the endpoint has at its disposal:')\n",
    "!aws s3 ls $model_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model NewYork_NY.tar.gz to predict price of this house: [1983, 4122, 6, 2.0, 0.88, 3]\n",
      "$544,632.56, took 877 ms\n",
      "\n",
      "Using model NewYork_NY.tar.gz to predict price of this house: [1992, 3794, 5, 3.0, 1.32, 2]\n",
      "$566,414.70, took 120 ms\n",
      "\n",
      "Using model NewYork_NY.tar.gz to predict price of this house: [1996, 1636, 3, 2.0, 0.92, 0]\n",
      "$230,567.33, took 115 ms\n",
      "\n",
      "Using model NewYork_NY.tar.gz to predict price of this house: [2001, 2524, 5, 1.0, 1.2, 1]\n",
      "$413,050.06, took 119 ms\n",
      "\n",
      "Using model NewYork_NY.tar.gz to predict price of this house: [1984, 3486, 4, 2.0, 0.75, 3]\n",
      "$451,767.35, took 121 ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "model_name = 'NewYork_NY'\n",
    "full_model_name = '{}.tar.gz'.format(model_name)\n",
    "for i in range(5):\n",
    "    features = gen_random_house()\n",
    "    predict_one_house_value(gen_random_house()[1:], full_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shut down the endpoint\n",
    "sm_client.delete_endpoint(EndpointName=endpoint_name)\n",
    "# and the endpoint config\n",
    "sm_client.delete_endpoint_config(EndpointConfigName=endpoint_config_name)\n",
    "# delete model too\n",
    "sm_client.delete_model(ModelName=multi_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
