
# Sagemaker Inference Pipelines

An inference pipeline is an Amazon SageMaker model that is composed of a linear sequence of two to five containers that process requests for inferences on data. You use an inference pipeline to define and deploy any combination of pretrained Amazon SageMaker built-in algorithms and your own custom algorithms packaged in Docker containers. You can use an inference pipeline to combine preprocessing, predictions, and post-processing data science tasks. Inference pipelines are fully managed.

The following code examples are provided:

- [Inference Pipeline with Scikit-learn and Linear Learner](scikit_learn_inference_pipeline) Code to demonstrate how you can build your ML Pipeline leveraging the Sagemaker Scikit-learn container and SageMaker Linear Learner algorithm & after the model is trained, deploy the Pipeline (Data preprocessing and Lineara Learner) as an Inference Pipeline behind a single Endpoint for real time inference and for batch inferences using Amazon SageMaker Batch Transform.

- [Feature processing with Spark, training with BlazingText and deploying as Inference Pipeline](inference_pipeline_sparkml_blazingtext_dbpedia) Code in this folder demonstrates how you can build your ML Pipeline leveraging Spark Feature Transformers and SageMaker BlazingText algorithm & after the model is trained, deploy the Pipeline (Feature Transformer and BlazingText) as an Inference Pipeline behind a single Endpoint for real-time inference and for batch inferences using Amazon SageMaker Batch Transform.
- [Feature processing with Spark, training with XGBoost and deploying as Inference Pipeline](inference_pipeline_sparkml_xgboost_abalone) Code in this folder demonstrates how you can build your ML Pipeline leveraging Spark Feature Transformers and SageMaker XGBoost algorithm & after the model is trained, deploy the Pipeline (Feature Transformer and XGBoost) as an Inference Pipeline behind a single Endpoint for real-time inference and for batch inferences using Amazon SageMaker Batch Transform.
- [Deploy Apache Spark pre-processing and post-processing with XGBoost for real-time prediction requests in Amazon SageMaker using Inference Pipelines](inference_pipeline_sparkml_xgboost_car_evaluation) SparkMl is used in a pipeline to predict the acceptability of a specific car, amongst the values of: unacc, acc, good, and vgood. SparkML pre-processing and post-processing scripts are written once, and applied for processing training data using AWS Glue. Serialize and capture the SparkML artifacts produced by AWS Glue to Amazon S3 using MLeap. This is so that they can be reused during inference for real-time requests using the SparkML Serving container that Amazon SageMaker provides. Finally, deploy the pre-processing, inference, and post-processing steps in an inference pipeline and to execute these steps in order for each real-time inference request.
