# ml-deployment

Code repository that was started for ML Innovate 2020 on the topic of ML Deployment. Watch out for this repo as more mode gets added to this repo to help developers deploy their Ml Solutions on AWS.

Most of the code in this repository is adapted from the [aws-sagemeaker-examples](https://github.com/awslabs/amazon-sagemaker-examples) repo.

The topics covered are:

- [Resource Profiling using CloudWatch Metrics](resource-profiling)
- [Load Testing Deployment Endpoint using Serverless-Artillery](loadTesting)
- [Offline Deployment Using Batch Transform](batch-transform)
- [Online Deployment Using Sagemaker Hosting](hosting)
- [Using Sagemaker Inference Pipelines to string together Pre, Inference, Post-processing steps as a single deployment](inference-pipelines)
- [Sagemaker Multi-model Deployment Endpoint](multi-model)
- [Using Elastic Inference for Flexibly adding GPU accelerators](elastic-inference)
- [Preparing Models in different frameworks for different target hardware using Sagemaker Neo](neo)
- [Deploying using tensorflow serving](tf-serving)
